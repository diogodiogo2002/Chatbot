```markdown
# ğŸ“š RAG Chatbot com LangChain, FastAPI e Widget Web

Este projeto Ã© um **chatbot com RAG (Retrieval-Augmented Generation)** que responde a perguntas baseando-se em documentos PDF carregados. O sistema utiliza **LangChain**, **ChromaDB**, **FastAPI** no backend, e um **widget simples em HTML/CSS/JS** no frontend.

---

## ğŸ§  Tecnologias Utilizadas

- [cite_start]**FastAPI** â€” API moderna e rÃ¡pida para backend [cite: 5]
- **LangChain** â€” Framework para RAG (RecuperaÃ§Ã£o + GeraÃ§Ã£o)
- [cite_start]**ChromaDB** â€” Base vetorial local para recuperaÃ§Ã£o semÃ¢ntica [cite: 5]
- [cite_start]**HuggingFace Embeddings** â€” Modelo de embeddings [cite: 5]
- [cite_start]**Together.ai API** â€” LLM Mistral-7B-Instruct para respostas [cite: 5]
- [cite_start]**HTML/CSS/JavaScript** â€” Interface do widget [cite: 3, 4]

---

## ğŸ“ Estrutura do Projeto

```

â”œâ”€ docs                \# PDF(s) a serem carregados
â”œâ”€interface/
|  â”œâ”€â”€ backend/
|  â”‚   â”œâ”€â”€ ingest\_database.py    \# IndexaÃ§Ã£o de documentos PDF
|  â”‚   â”œâ”€â”€ main.py               \# Backend FastAPI com RAG
|  â”‚   â””â”€â”€ chroma\_db/            \# Base de dados vetorial persistente
|  â”œâ”€â”€ frontend/
|  â”‚   â”œâ”€â”€ index.html            \# Interface do widget
|  â”‚   â”œâ”€â”€ style.css             \# Estilos visuais
|  â”‚   â””â”€â”€ script.js             \# LÃ³gica do frontend
â”œâ”€ .env                \# Chave TOGETHER\_API\_KEY
â”œâ”€ README.md           \# Este ficheiro

````

---

## âš™ï¸ InstalaÃ§Ã£o

### 1. PrÃ©-requisitos

Certifique-se de ter **Python 3.8+** e **pip** instalados.

### 2. Clonar o RepositÃ³rio

```bash
git clone <URL_DO_SEU_REPOSITORIO>
cd <nome_da_pasta_do_repositorio>
````

### 3\. Criar e Ativar o Ambiente Virtual

```bash
python -m venv venv
```

**No Windows:**

```bash
.\venv\Scripts\activate
```

**No macOS/Linux:**

```bash
source venv/bin/activate
```

### 4\. Instalar dependÃªncias

Crie um arquivo `requirements.txt` na raiz do projeto com o seguinte conteÃºdo:

```
fastapi
uvicorn
python-dotenv
langchain
langchain-chroma
langchain-community
langchain-huggingface
langchain-openai
pydantic
PyPDF2
sentence-transformers
```

EntÃ£o, instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

### 5\. Criar o ficheiro `.env`

Crie um ficheiro `.env` na raiz do seu projeto com a sua chave da API da Together.ai:

```
TOGETHER_API_KEY=<SUA_CHAVE_AQUI>
```

Substitua `<SUA_CHAVE_AQUI>` pela sua chave real obtida em [Together.ai](https://www.together.ai/).

### 6\. Colocar os PDFs

Coloque os seus documentos PDF na pasta `docs/`. O chatbot irÃ¡ usar estes documentos como base para as respostas.

### 7\. Indexar os documentos

Navegue atÃ© a pasta `interface/backend` e execute o script de ingestÃ£o:

```bash
cd interface/backend
python ingest_database.py
```

Este passo cria a base de dados vetorial `chroma_db/`.

### 8\. Iniciar o backend

Na pasta `interface/backend`, inicie o servidor FastAPI:

```bash
uvicorn main:app --reload
```

O servidor estarÃ¡ acessÃ­vel em `http://127.0.0.1:8000`.

-----

## ğŸ’¬ Como Funciona

### Passo 1: IngestÃ£o

[cite\_start]O script `ingest_database.py`[cite: 1]:

  - [cite\_start]LÃª os documentos PDF da pasta `docs/`[cite: 1].
  - [cite\_start]Divide os documentos em trechos (chunks)[cite: 1].
  - [cite\_start]Cria embeddings para esses trechos usando o modelo `sentence-transformers/all-MiniLM-L6-v2`[cite: 1].
  - [cite\_start]Armazena esses embeddings numa base de dados vetorial persistente chamada `chroma_db/`[cite: 1].

### Passo 2: Chat

[cite\_start]Quando o utilizador envia uma pergunta atravÃ©s da interface web[cite: 3]:

1.  [cite\_start]O backend recebe a mensagem via um pedido `POST` para `/chat`[cite: 5].
2.  [cite\_start]O sistema recupera os 5 trechos mais relevantes da base de dados vetorial (`chroma_db`) com base na pergunta do utilizador[cite: 5].
3.  [cite\_start]Um prompt de RAG Ã© construÃ­do, combinando a pergunta do utilizador com o conhecimento recuperado dos documentos[cite: 5].
4.  [cite\_start]O modelo de linguagem `Mistral-7B-Instruct-v0.1` (via Together.ai API) gera uma resposta com base neste prompt[cite: 5].
5.  [cite\_start]A resposta Ã© enviada de volta para a interface do utilizador[cite: 5].

-----

## ğŸ§‘â€ğŸ’» Frontend

A pasta `interface/frontend` contÃ©m os seguintes ficheiros:

  - [cite\_start]`index.html`: A estrutura principal do widget do chatbot[cite: 4].
  - [cite\_start]`style.css`: Define os estilos visuais para o chatbot, como cores, tamanhos e layout[cite: 2].
  - [cite\_start]`script.js`: ContÃ©m a lÃ³gica JavaScript que gere a interaÃ§Ã£o do utilizador, a abertura/fecho do widget e a comunicaÃ§Ã£o com o backend FastAPI[cite: 3].

âš ï¸ **O widget comunica com `http://127.0.0.1:8000/chat` â€” garanta que o backend estÃ¡ ativo e acessÃ­vel nesta porta.**

-----

## ğŸ›¡ï¸ SeguranÃ§a e ProduÃ§Ã£o

  * **CORS (Cross-Origin Resource Sharing):** No arquivo `main.py`, a configuraÃ§Ã£o de CORS (`app.add_middleware`) estÃ¡ definida para `allow_origins=["*"]` para facilitar o desenvolvimento. Em produÃ§Ã£o, **recomenda-se fortemente** que esta configuraÃ§Ã£o seja alterada para permitir apenas o domÃ­nio especÃ­fico do seu frontend, aumentando a seguranÃ§a.
  * **Chave de API:** Mantenha a sua `TOGETHER_API_KEY` segura e nunca a inclua diretamente no cÃ³digo-fonte ou faÃ§a commit em repositÃ³rios pÃºblicos. O uso do `.env` Ã© uma boa prÃ¡tica.

-----

## âœ… Exemplo de uso

```bash
# 1. Coloque os seus PDFs na pasta 'docs/'
# 2. Navegue atÃ© 'interface/backend' e execute: python ingest_database.py
# 3. Na mesma pasta 'interface/backend', inicie o backend: uvicorn main:app --reload
# 4. Abra o ficheiro 'interface/frontend/index.html' no seu navegador
# 5. Clique no Ã­cone do chat e comece a fazer perguntas relacionadas aos seus PDFs!
```

-----

## ğŸ§¾ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Sinta-se Ã  vontade para modificar e usar.

-----

## âœ¨ Feito por

Rodrigo GonÃ§alves e equipa ğŸš€

```
```
